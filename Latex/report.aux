\relax 
\citation{DetectMisinformation1}
\citation{DetectMisinformation2}
\citation{DetectMisinformation3}
\citation{DetectMisinformation4}
\citation{PropagandaAnalysis1}
\citation{PropagandaAnalysis2}
\citation{PropagandaAnalysis3}
\citation{NaiveLearning1}
\citation{NaiveLearning2}
\citation{NaiveLearning3}
\citation{A2C_1}
\citation{A2C_3}
\citation{A3C_1}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{random_graphs}
\citation{NaiveLearning1}
\citation{NaiveLearning2}
\citation{NaiveLearning3}
\citation{viral_marketing1}
\citation{viral_marketing_2}
\citation{viral_marketing_3}
\@writefile{toc}{\contentsline {section}{\numberline {2}Modeling the Social Network}{2}}
\newlabel{sec:SN}{{2}{2}}
\newlabel{equ:update}{{4}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Social Network Model as a Reinforcement Learning Environment}{3}}
\newlabel{sec:SN_RLEnv}{{3}{3}}
\citation{large_action_2}
\citation{large_action_1}
\citation{rlblogpost}
\citation{NaiveLearning1}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Observation Space}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Action Space}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Reward Function}{4}}
\citation{gym}
\citation{A2C_1}
\citation{A3C_1}
\citation{A3C_2}
\citation{A3C_1}
\citation{A2C_3}
\citation{A2C_1}
\citation{stable-baselines}
\newlabel{equ:s_est}{{6}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Iteration step reward based on the difference between estimated and true source value.\relax }}{5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:reward_func}{{1}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Training an agent with A2C}{5}}
\newlabel{sec:A2C}{{4}{5}}
\citation{A2C_2}
\citation{pol_grad2}
\citation{pol_grad1}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Advantage Actor Critic}{6}}
\newlabel{equ:pol_grad}{{10}{6}}
\citation{A3C_1}
\citation{entropy_1}
\citation{A3C_1}
\citation{A3C_1}
\newlabel{equ:pol_grad_fin}{{15}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Entropy in Advantage Actor Critic}{7}}
\citation{gnn_1}
\citation{gnn_2}
\citation{gnn_3}
\citation{rl_gnn2}
\citation{rl_gnn1}
\citation{RMSprop}
\citation{rlblogpost}
\newlabel{equ:pol_grad_fin}{{16}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Network Architecture}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Network architecture for the actor and the critic. Both share the first 3 layers. The numbers in the layers indicate amount of nodes. All layers are fully connected.\relax }}{8}}
\newlabel{fig:NN_arch}{{2}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Pre-Training}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{9}}
\newlabel{sec:results}{{5}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Performance for varying Levels of Misinformation}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Comparison of discounted episode rewards between an agent that randomly chooses actions on the interval $[0,1]^{100}$ and the do-nothing agent for different ratios of true and false sources. Mean value over 300 episodes is taken.\relax }}{10}}
\newlabel{fig:comp_dnA_randomA}{{3}{10}}
\newlabel{tab:perf_1}{{5.1}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Discounted episode rewards for the baseline do-nothing agent, the pre-trained model and the fully trained, complete model for different ratios of true and false sources. Mean values over 300 runs are taken.\relax }}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Comparison of discounted episode rewards between the fully trained model and the do-nothing agent for different ratios of true and false sources.\relax }}{11}}
\newlabel{fig:per_cm}{{4}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Difference in discounted episode rewards between pre-trained model and fully trained model for different ratio of false and true sources.\relax }}{12}}
\newlabel{fig:dif_cm-pto}{{5}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Behavior of Trained Agent}{12}}
\newlabel{sec:res_behavior}{{5.2}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces First part of an observation and corresponding action of the fully trained agent. There are 7 true and 3 false sources. This observation was created in the first iteration. Only the first 10 of 100 values are shown.\relax }}{12}}
\newlabel{fig:act-obs_7t3f}{{6}{12}}
\newlabel{fig:3t7f_obs_iter1}{{7a}{13}}
\newlabel{sub@fig:3t7f_obs_iter1}{{a}{13}}
\newlabel{fig:3t7f_action_iter1}{{7b}{13}}
\newlabel{sub@fig:3t7f_action_iter1}{{b}{13}}
\newlabel{fig:3t7f_obs_iter2}{{7c}{13}}
\newlabel{sub@fig:3t7f_obs_iter2}{{c}{13}}
\newlabel{fig:3t7f_action_iter2}{{7d}{13}}
\newlabel{sub@fig:3t7f_action_iter2}{{d}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Observation and corresponding action for 2 iterations. There are 7 false and 3 true sources. The true source value is $s_T=0.10$.\relax }}{13}}
\newlabel{fig:act-obs_3t7f}{{7}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Convergence}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Episode rewards over the 5 million training iterations for social network with 6 true and 4 false sources. Figure (a) is smoothed with a factor of 0.6 and (b) is smoothed by a factor of 0.99.\relax }}{14}}
\newlabel{fig:reward_tb}{{8}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Episode loss over the 5 million training iterations. Figure (a) is smoothed with a factor of 0.6 and (b) is smoothed by a factor of 0.99.\relax }}{14}}
\newlabel{fig:loss_tb}{{9}{14}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions}{15}}
\bibdata{references}
\bibcite{PropagandaAnalysis1}{1}
\bibcite{rl_gnn2}{2}
\bibcite{NaiveLearning3}{3}
\bibcite{A3C_2}{4}
\bibcite{viral_marketing1}{5}
\bibcite{pol_grad1}{6}
\bibcite{gym}{7}
\bibcite{viral_marketing_3}{8}
\bibcite{large_action_2}{9}
\bibcite{large_action_1}{10}
\bibcite{PropagandaAnalysis3}{11}
\bibcite{random_graphs}{12}
\bibcite{A2C_2}{13}
\bibcite{DetectMisinformation3}{14}
\bibcite{stable-baselines}{15}
\bibcite{NaiveLearning2}{16}
\bibcite{rlblogpost}{17}
\bibcite{NaiveLearning1}{18}
\bibcite{DetectMisinformation1}{19}
\bibcite{A3C_1}{20}
\bibcite{DetectMisinformation4}{21}
\bibcite{pol_grad2}{22}
\bibcite{RMSprop}{23}
\bibcite{rl_gnn1}{24}
\bibcite{A2C_1}{25}
\bibcite{entropy_1}{26}
\bibcite{PropagandaAnalysis2}{27}
\bibcite{A2C_3}{28}
\bibcite{gnn_3}{29}
\bibcite{gnn_1}{30}
\bibcite{DetectMisinformation2}{31}
\bibcite{gnn_2}{32}
\bibcite{viral_marketing_2}{33}
\bibstyle{plain}
