\relax 
\citation{random_graphs}
\citation{NaiveLearning1}
\citation{NaiveLearning2}
\citation{NaiveLearning3}
\citation{viral_marketing1}
\citation{viral_marketing_2}
\citation{viral_marketing_3}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Modeling the Social Network}{1}}
\newlabel{sec:SN}{{2}{1}}
\newlabel{equ:update}{{4}{2}}
\citation{large_action_2}
\citation{large_action_1}
\citation{reward_design_1}
\citation{reward_design_2}
\citation{NaiveLearning1}
\@writefile{toc}{\contentsline {section}{\numberline {3}Social Network Model as a Reinforcement Learning Environment}{3}}
\newlabel{sec:SN_RLEnv}{{3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Observation Space}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Action Space}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Reward Function}{3}}
\citation{gym}
\citation{A2C_1}
\citation{A3C_1}
\citation{A3C_2}
\citation{A3C_1}
\citation{A2C_3}
\citation{A2C_1}
\citation{stable-baselines}
\newlabel{equ:s_est}{{6}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Iteration step reward based on the difference between estimated and true source value.\relax }}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:reward_func}{{1}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Training an agent with A2C}{4}}
\citation{A2C_2}
\citation{pol_grad2}
\citation{pol_grad1}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Advantage Actor Critic}{5}}
\newlabel{equ:pol_grad}{{10}{5}}
\citation{A3C_1}
\citation{entropy_1}
\citation{A3C_1}
\citation{A3C_1}
\newlabel{equ:pol_grad_fin}{{15}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Entropy in Advantage Actor Critic}{6}}
\citation{gnn_1}
\citation{gnn_2}
\citation{gnn_3}
\citation{rl_gnn2}
\citation{rl_gnn1}
\citation{RMSprop}
\newlabel{equ:pol_grad_fin}{{16}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Network Architecture}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Network architecture for the actor and the critic. Both share the first 3 layers. The numbers in the layers indicate amount of nodes. All layers are fully connected.\relax }}{7}}
\newlabel{fig:NN_arch}{{2}{7}}
\citation{rlblogpost}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Pre-Training}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Performance for varying Levels of Misinformation}{9}}
\newlabel{tab:perf_1}{{5.1}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Discounted rewards for the baseline do-nothing agent, the pre-trained model and the fully trained, complete model for different ratio of true and false sources. Mean value over 300 runs.\relax }}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Comparison between the fully trained model and the do-nothing agent for different ratios of true and false sources.\relax }}{10}}
\newlabel{fig:per_cm}{{3}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Difference between pre-trained model and fully trained model for different ratio of false and true sources.\relax }}{10}}
\newlabel{fig:dif_cm-pto}{{4}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Behavior of Trained Agent}{11}}
\newlabel{sec:res_behavior}{{5.2}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces First part of observation and corresponding action of the fully trained agent. There are 7 true and 3 false sources. This observation was created in the first iteration. Only the first 10 of 100 values are shown.\relax }}{11}}
\newlabel{fig:act-obs_7t3f}{{5}{11}}
\newlabel{fig:3t7f_obs_iter1}{{6a}{12}}
\newlabel{sub@fig:3t7f_obs_iter1}{{a}{12}}
\newlabel{fig:3t7f_action_iter1}{{6b}{12}}
\newlabel{sub@fig:3t7f_action_iter1}{{b}{12}}
\newlabel{fig:3t7f_obs_iter2}{{6c}{12}}
\newlabel{sub@fig:3t7f_obs_iter2}{{c}{12}}
\newlabel{fig:3t7f_action_iter2}{{6d}{12}}
\newlabel{sub@fig:3t7f_action_iter2}{{d}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Observation and corresponding action for 2 iterations. There are 7 false and 3 true sources. The true source value is $s_T=0.10$.\relax }}{12}}
\newlabel{fig:act-obs_3t7f}{{6}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Convergence}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces .\relax }}{13}}
\newlabel{fig:reward_tb}{{7}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces .\relax }}{13}}
\newlabel{fig:loss_tb}{{8}{13}}
\bibdata{references}
\bibcite{rl_gnn2}{1}
\bibcite{NaiveLearning3}{2}
\bibcite{reward_design_1}{3}
\bibcite{A3C_2}{4}
\bibcite{viral_marketing1}{5}
\bibcite{pol_grad1}{6}
\bibcite{gym}{7}
\bibcite{viral_marketing_3}{8}
\bibcite{large_action_2}{9}
\bibcite{large_action_1}{10}
\bibcite{random_graphs}{11}
\bibcite{A2C_2}{12}
\bibcite{stable-baselines}{13}
\bibcite{NaiveLearning2}{14}
\bibcite{rlblogpost}{15}
\bibcite{NaiveLearning1}{16}
\bibcite{reward_design_2}{17}
\bibcite{A3C_1}{18}
\bibcite{pol_grad2}{19}
\bibcite{RMSprop}{20}
\bibcite{rl_gnn1}{21}
\bibcite{A2C_1}{22}
\bibcite{entropy_1}{23}
\bibcite{A2C_3}{24}
\bibcite{gnn_3}{25}
\bibcite{gnn_1}{26}
\bibcite{gnn_2}{27}
\bibcite{viral_marketing_2}{28}
\bibstyle{plain}
